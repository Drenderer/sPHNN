{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thermal food processing surrogate\n",
    "Part A: Varying number of augmented dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../..')   # Allow relative imports from the parent folder\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import jax.random as jr\n",
    "from jaxtyping import Array\n",
    "\n",
    "import optax\n",
    "\n",
    "import equinox as eqx\n",
    "\n",
    "from dynax import training\n",
    "from dynax import evaluation\n",
    "from dynax.function_models import LyapunovNN, OnsagerNetPotential, FICNN, \\\n",
    "                                  ConstantSkewSymmetricMatrix, \\\n",
    "                                  ConstantMatrix, ConstantSPDMatrix, \\\n",
    "                                  MLP\n",
    "from dynax.integration_models import ODESolver\n",
    "from dynax.derivative_models import BaseModel, ISPHS\n",
    "from dynax.losses import mse\n",
    "from dynax.sphnn_tools import is_zero_gas_guarantee_valid\n",
    "from dynax.data_handling import Normalizer, NormalizationWrapper\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from util import colors\n",
    "\n",
    "ACTIVATIONS = dict(\n",
    "    softplus = jax.nn.softplus,\n",
    "    tanh = jax.nn.tanh,\n",
    "    relu = jax.nn.relu,\n",
    ")\n",
    "\n",
    "INITIALIZERS = dict(\n",
    "    he_uniform = jax.nn.initializers.he_uniform(),\n",
    "    glorot_uniform = jax.nn.initializers.glorot_uniform(),\n",
    "    zeros = jax.nn.initializers.zeros,\n",
    ")\n",
    "\n",
    "### Matplotlib Settings \n",
    "SMALL_SIZE = 7\n",
    "MEDIUM_SIZE = 8\n",
    "BIGGER_SIZE = 10\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "colors.set_custom_cycle()\n",
    "\n",
    "model_style = {\n",
    "    'true':     dict(c='black',                          ls='--',  lw=2.0, alpha=1.0),\n",
    "    'sPHNN':    dict(c=colors.theme_colors['red'],       ls='-',   lw=2.0, alpha=0.8),\n",
    "    'PHNN':     dict(c=colors.theme_colors['green'],     ls=':',   lw=1.0, alpha=0.8),\n",
    "    'NODE':     dict(c=colors.theme_colors['lightblue'], ls='-',   lw=1.0, alpha=0.8),\n",
    "    'cPHNN':    dict(c=colors.theme_colors['orange'],    ls='-.',  lw=1.0, alpha=0.8),\n",
    "}\n",
    "\n",
    "model_names = {\n",
    "    'true':     'true',\n",
    "    'sPHNN':    'sPHNN',\n",
    "    'PHNN':     'PHNN',\n",
    "    'NODE':     'NODE',    \n",
    "    'cPHNN':    'bPHNN',   \n",
    "}\n",
    "\n",
    "PLOT_DPI = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and training definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_sPHNN_model(model_hyperparams: dict,\n",
    "                       num_aug: int,\n",
    "                       *, key):\n",
    "\n",
    "    j_key, r_key, g_key, h_key= jr.split(key, 4)\n",
    "\n",
    "    state_size, input_size = 2 + num_aug, 1\n",
    "\n",
    "    J = ConstantSkewSymmetricMatrix(state_size, key=j_key)\n",
    "    R = ConstantSPDMatrix(state_size, initializer=jnn.initializers.zeros, key=r_key)\n",
    "    g = ConstantMatrix((state_size, input_size), initialize=jnn.initializers.zeros, key=g_key)\n",
    "\n",
    "    ficnn = FICNN(state_size, 'scalar', \n",
    "                  width=model_hyperparams['ficnn_width'],\n",
    "                  depth=model_hyperparams['ficnn_depth'],\n",
    "                  activation=jnn.softplus,\n",
    "                  w_initializer=model_hyperparams['weight_initialization'], # Hardcoded old version: jnn.initializers.glorot_uniform(),\n",
    "                  b_initializer=model_hyperparams['bias_initialization'],   # Hardcoded old version: jnn.initializers.normal(),\n",
    "                  key=h_key)\n",
    "    H = LyapunovNN(ficnn, minimum=jnp.zeros(state_size))\n",
    "    sphnn_ = ISPHS(H, J, R, g)\n",
    "    sphnn_ode = ODESolver(sphnn_, augmentation=num_aug, augmentation_learnable=False)\n",
    "\n",
    "    return sphnn_ode\n",
    "\n",
    "def define_PHNN_model(model_hyperparams: dict,\n",
    "                      num_aug: int,\n",
    "                      *, key):\n",
    "\n",
    "    j_key, r_key, g_key, h_key= jr.split(key, 4)\n",
    "\n",
    "    state_size, input_size = 2 + num_aug, 1\n",
    "\n",
    "    J = ConstantSkewSymmetricMatrix(state_size, key=j_key)\n",
    "    R = ConstantSPDMatrix(state_size, initializer=jnn.initializers.zeros, key=r_key)\n",
    "    g = ConstantMatrix((state_size, input_size), initialize=jnn.initializers.zeros, key=g_key)\n",
    "\n",
    "    H = MLP(in_size=state_size, out_size='scalar', \n",
    "            width_size=model_hyperparams['mlp_width'],\n",
    "            depth=model_hyperparams['mlp_depth'],\n",
    "            weight_initializer=model_hyperparams['weight_initialization'],\n",
    "            bias_initializer=model_hyperparams['bias_initialization'],  \n",
    "            activation=model_hyperparams['activation'], # Old hardcoded version: jnn.tanh,\n",
    "            key=h_key)\n",
    "    phnn_ = ISPHS(H, J, R, g)\n",
    "    phnn_ode_ = ODESolver(phnn_, augmentation=num_aug, augmentation_learnable=True) # 1 Parameters\n",
    "\n",
    "    return phnn_ode_\n",
    "\n",
    "def define_NODE_model(model_hyperparams: dict,\n",
    "                      num_aug: int,\n",
    "                      *, key):\n",
    "    \n",
    "    state_size, input_size = 2 + num_aug, 1\n",
    "\n",
    "    mlp = MLP(state_size+input_size, state_size, \n",
    "              width_size=model_hyperparams['mlp_width'],\n",
    "              depth=model_hyperparams['mlp_depth'],\n",
    "              weight_initializer=model_hyperparams['weight_initialization'],\n",
    "              bias_initializer=model_hyperparams['bias_initialization'],  \n",
    "              activation=model_hyperparams['activation'], # Old hardcoded version: jnn.tanh,\n",
    "              key=key)\n",
    "    div_model = BaseModel(mlp, state_size, input_size)\n",
    "    node = ODESolver(div_model, augmentation=num_aug, augmentation_learnable=True,\n",
    "                     max_steps=8192,    # Hopefully all nodes can be integrated by increasing the max steps.\n",
    "                     )\n",
    "\n",
    "    return node\n",
    "\n",
    "def define_cPHNN_model(model_hyperparams: dict,\n",
    "                       num_aug: int,\n",
    "                       *, key):\n",
    "\n",
    "    j_key, r_key, g_key, h_key= jr.split(key, 4)\n",
    "\n",
    "    state_size, input_size = 2 + num_aug, 1\n",
    "\n",
    "    J = ConstantSkewSymmetricMatrix(state_size, key=j_key)\n",
    "    R = ConstantSPDMatrix(state_size, initializer=jnn.initializers.zeros, key=r_key)\n",
    "    g = ConstantMatrix((state_size, input_size), initialize=jnn.initializers.zeros, key=g_key)\n",
    "\n",
    "    H = OnsagerNetPotential(\n",
    "        state_size, \n",
    "        width_size=model_hyperparams['mlp_width'],\n",
    "        depth=model_hyperparams['mlp_depth'],\n",
    "        activation=model_hyperparams['activation'],\n",
    "        weight_initializer=model_hyperparams['weight_initialization'],\n",
    "        bias_initializer=model_hyperparams['bias_initialization'],  \n",
    "        beta=model_hyperparams['beta'], \n",
    "        beta_learnable=model_hyperparams['beta_learnable'], \n",
    "        key=h_key\n",
    "    )\n",
    "\n",
    "    cphnn_ = ISPHS(H, J, R, g)\n",
    "    cphnn_ode_ = ODESolver(cphnn_, augmentation=num_aug, augmentation_learnable=True)\n",
    "\n",
    "    return cphnn_ode_\n",
    "\n",
    "\n",
    "def get_rmse(model, ts, ys, us):\n",
    "    ys_pred = jax.vmap(model, in_axes=(None, 0, 0))(ts, ys[:,0], us)\n",
    "    return jnp.sqrt(jnp.mean(jnp.square(ys_pred - ys)))\n",
    "\n",
    "def train_template(model, \n",
    "                   training_data: tuple, validation_data: tuple|None,\n",
    "                   training_hyperparams: dict,\n",
    "                   weights_dir=None, overwrite=False,\n",
    "                   loss_fn = mse,\n",
    "                   *, key):\n",
    "\n",
    "    try:\n",
    "        model = training.load_weights(weights_dir/Path('weights.eqx'), model)\n",
    "        history = training.load_history(weights_dir/Path('history.npz'))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        model, history = training.fit_trajectory(\n",
    "            model, *training_data,\n",
    "            validation_data = validation_data,\n",
    "            loss_fn         = loss_fn,\n",
    "            batch_size      = training_hyperparams['batch_size'],\n",
    "            steps           = training_hyperparams['steps'],\n",
    "            optimizer       = optax.adam(training_hyperparams['learning_rate']),\n",
    "            log_loss_every  = 100,\n",
    "            callback        = training.save_every(5000, weights_dir/'checkpoint'),\n",
    "            key             = key\n",
    "            )\n",
    "        \n",
    "        training.save_weights(weights_dir/Path('weights'), model, overwrite)\n",
    "        training.save_history(weights_dir/Path('history'), history, overwrite)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def evaluation_template(model, \n",
    "                        training_data: tuple, test_data: tuple,\n",
    "                        metrics_dir: Path):\n",
    "    \n",
    "    # Load RMSE data if available, otherwise compute it\n",
    "    try:\n",
    "        error_measures = np.load(metrics_dir/Path('error_measures.npz'))\n",
    "        error_measures = dict(error_measures)\n",
    "    except FileNotFoundError:\n",
    "        error_measures = {\n",
    "            'test_rmse': get_rmse(model, *test_data),\n",
    "            'train_rmse': get_rmse(model, *training_data),\n",
    "        }\n",
    "        np.savez(metrics_dir/Path('error_measures.npz'), **error_measures)\n",
    "\n",
    "    return error_measures\n",
    "\n",
    "\n",
    "def get_prediction_statistics(ts: Array, ys: Array, us: Array, results:dict, error_metric=evaluation.rmse, exclude_model_types=[]):\n",
    "\n",
    "    # Add batch dimension if only a single trajectory is passed\n",
    "    ys = evaluation.make_batched(ys)\n",
    "    us = evaluation.make_batched(us)\n",
    "\n",
    "    true_data = dict(\n",
    "        ts = ts, ys = ys, us = us\n",
    "    )\n",
    "    prediction_data = {}\n",
    "    for model_type, model_results in results.items():\n",
    "        if model_type in exclude_model_types:\n",
    "            continue\n",
    "        \n",
    "        prediction_data[model_type] = {}\n",
    "        for aug_dim, aug_results in model_results.items():\n",
    "            ys_preds = []\n",
    "            errors = []\n",
    "            for n, instance_result in enumerate(aug_results):\n",
    "                model = instance_result['wrapped_model']\n",
    "                try:\n",
    "                    ys_pred = jax.vmap(model, in_axes=(None, 0, 0))(ts, ys[:,0], us)\n",
    "                    ys_preds.append(ys_pred)\n",
    "                    errors.append(error_metric(ys_pred, ys))\n",
    "                except:\n",
    "                    print(f'{model_type}, instance {n} faild to integrate.')\n",
    "                \n",
    "            ys_preds = np.stack(ys_preds, axis=0)\n",
    "            errors = np.stack(errors, axis=0)\n",
    "            print(f'# of valid {model_type} predictions: {ys_preds.shape[0]}')\n",
    "            prediction_data[model_type][aug_dim] = dict(\n",
    "                ys      = evaluation.get_statistics(ys_preds),\n",
    "                errors  = evaluation.get_statistics(errors),\n",
    "            )\n",
    "    return prediction_data, true_data\n",
    "\n",
    "def get_hyperparams(file: Path, default_hyperparamse: dict):\n",
    "    file = Path(file)\n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            hyperparams = json.load(f)\n",
    "        return hyperparams\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(file, 'w') as f:\n",
    "            json.dump(default_hyperparamse, f, indent=4)\n",
    "        return default_hyperparamse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [745, 795]\n",
    "test_ids  = [313, 320, 344, 378, 383, 407, 412, 415, 461, 462, 466, 467, 474, 508, 528] # Test group AP15\n",
    "\n",
    "data = np.load('../../data/thermal_food_processing_surrogate/data.npz')\n",
    "ts_train, ys_train, us_train = data['ts_train'], data['ys_train'][:2], data['us_train'][:2]\n",
    "ts_vali,  ys_vali,  us_vali  = data['ts_vali'],  data['ys_vali'],  data['us_vali'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_shift = 0.\n",
    "ts_scale = jnp.std(ts_train - ts_shift)\n",
    "t_normalizer = Normalizer(ts_shift, ts_scale)\n",
    "\n",
    "ys_shift = 279.15\n",
    "ys_scale = jnp.std(ys_train - ys_shift)\n",
    "y_normalizer = Normalizer(ys_shift, ys_scale)\n",
    "\n",
    "us_shift = 279.15\n",
    "us_scale = jnp.std(us_train - us_shift)\n",
    "u_normalizer = Normalizer(us_shift, us_scale)\n",
    "\n",
    "# Compute the normalized data\n",
    "def get_normalized(ts, ys, us):\n",
    "    ts_norm  = t_normalizer.normalize(ts)\n",
    "    ys_norm  = y_normalizer.normalize(ys)\n",
    "    us_norm  = u_normalizer.normalize(us)\n",
    "    return ts_norm, ys_norm, us_norm\n",
    "\n",
    "train_data_norm = get_normalized(ts_train, ys_train, us_train)\n",
    "ts_vali_norm, ys_vali_norm, us_vali_norm  = get_normalized(ts_vali,  ys_vali,  us_vali )\n",
    "vali_data_norm = ((ts_vali_norm, ys_vali_norm[:,0], us_vali_norm), ys_vali_norm)\n",
    "train_data = (ts_train, ys_train, us_train)\n",
    "test_data  = (ts_vali,  ys_vali,  us_vali )\n",
    "\n",
    "# Function to wrap trained models\n",
    "def attach_normalizer(model: eqx.Module):\n",
    "    return NormalizationWrapper(model, t_normalizer, y_normalizer, u_normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters\n",
    "The following sets the hyperparameters for the experiment and/or the directory to save/load the trained models to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of the files for saving or loading\n",
    "save_dir = Path(R'results/run_A0')\n",
    "\n",
    "default_hyperparams = {\n",
    "    'meta': dict(\n",
    "        description = 'First attempt at final result',\n",
    "        num_instances = 20,\n",
    "        max_range_augmentations = 3,\n",
    "    ),\n",
    "    'sPHNN': dict(\n",
    "        ficnn_width             = 16,\n",
    "        ficnn_depth             = 2,\n",
    "        weight_initialization   = 'glorot_uniform',\n",
    "        bias_initialization     = 'zeros',\n",
    "    ),\n",
    "    'PHNN': dict(\n",
    "        mlp_width               = 16,\n",
    "        mlp_depth               = 2,\n",
    "        weight_initialization   = 'glorot_uniform',\n",
    "        bias_initialization     = 'zeros',\n",
    "        activation              = 'softplus'\n",
    "    ),\n",
    "    'NODE': dict(\n",
    "        mlp_width               = 16,\n",
    "        mlp_depth               = 2,\n",
    "        weight_initialization   = 'glorot_uniform',\n",
    "        bias_initialization     = 'zeros',\n",
    "        activation              = 'softplus'\n",
    "    ),\n",
    "    'cPHNN': dict(\n",
    "        mlp_width               = 16,\n",
    "        mlp_depth               = 2,\n",
    "        weight_initialization   = 'glorot_uniform',\n",
    "        bias_initialization     = 'zeros',\n",
    "        activation              = 'softplus',\n",
    "        beta                    = 0.1,\n",
    "        beta_learnable          = False,\n",
    "    ),\n",
    "    'training': dict(\n",
    "        batch_size      = 5,\n",
    "        steps           = 30_000,\n",
    "        learning_rate   = 1e-4,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Load hyperparameters / Save hyperparameters if new experiment\n",
    "hyperparams = get_hyperparams(save_dir/'hyperparameters.json', default_hyperparams)\n",
    "\n",
    "# Substitute the activations and intialization schemes\n",
    "for model in hyperparams.values():\n",
    "    if 'activation' in model.keys():\n",
    "        model['activation'] = ACTIVATIONS[model['activation']]\n",
    "    if 'weight_initialization' in model.keys():\n",
    "        model['weight_initialization'] = INITIALIZERS[model['weight_initialization']]\n",
    "    if 'bias_initialization' in model.keys():\n",
    "        model['bias_initialization'] = INITIALIZERS[model['bias_initialization']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train (or load) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definitions = {\n",
    "    'sPHNN':    partial(define_sPHNN_model, \n",
    "                        model_hyperparams      = hyperparams['sPHNN']),\n",
    "    'cPHNN':    partial(define_cPHNN_model, \n",
    "                        model_hyperparams      = hyperparams['cPHNN']),\n",
    "    'PHNN':     partial(define_PHNN_model,\n",
    "                        model_hyperparams      = hyperparams['PHNN']),\n",
    "    'NODE':     partial(define_NODE_model,\n",
    "                        model_hyperparams      = hyperparams['NODE']),\n",
    "}\n",
    "\n",
    "# Complete training definition\n",
    "train_model = partial(train_template,\n",
    "                      training_data = train_data_norm,\n",
    "                      validation_data = vali_data_norm,\n",
    "                      training_hyperparams = hyperparams['training'],\n",
    "                      loss_fn = mse)\n",
    "\n",
    "evaluate_model = partial(evaluation_template,\n",
    "                         training_data = train_data,\n",
    "                         test_data = test_data)\n",
    "\n",
    "# Load/Train the models\n",
    "num_instances = hyperparams['meta']['num_instances']\n",
    "results = {}\n",
    "for model_name, definition in model_definitions.items():\n",
    "    print(f'{model_name:-^40}')\n",
    "\n",
    "    aug_results = {}\n",
    "    for num_aug in range(hyperparams['meta']['max_range_augmentations'] + 1):\n",
    "        print(f'{num_aug:+^40}')\n",
    "\n",
    "        def train_instance(instance_id: int):\n",
    "            key = jr.key(instance_id)\n",
    "            model_key, loader_key = jr.split(key, 2)\n",
    "\n",
    "            # Define the model\n",
    "            _model = definition(num_aug=num_aug, key=model_key)\n",
    "\n",
    "            # Train or load the model\n",
    "            weights_dir=save_dir/model_name/f'augment_{num_aug}'/f'instance_{instance_id}'\n",
    "            _model, history = train_model(_model,\n",
    "                                        weights_dir=weights_dir,\n",
    "                                        key=loader_key)\n",
    "            model = training.resolve_constraints(_model)\n",
    "\n",
    "            # Wrap the model\n",
    "            wrapped_model = attach_normalizer(model)\n",
    "\n",
    "            # Evaluate the model\n",
    "            error_measures = evaluate_model(wrapped_model, \n",
    "                                            metrics_dir=weights_dir)\n",
    "\n",
    "            instance_result = dict(\n",
    "                model = model,\n",
    "                wrapped_model = wrapped_model,\n",
    "                history = history,\n",
    "            ) | error_measures\n",
    "\n",
    "            return instance_result\n",
    "        \n",
    "        aug_results[num_aug] = []\n",
    "        for i in range(num_instances):\n",
    "            aug_results[num_aug].append(train_instance(i))\n",
    "\n",
    "    results[model_name] = aug_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if all sPHNNs are 0-GAS (Globally asymptotically stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['sPHNN']:\n",
    "    all_passed = True\n",
    "    for num_aug, aug_results in results[model_name].items():\n",
    "        for n, instance_result in enumerate(aug_results):\n",
    "            isphs = instance_result['model'].derivative_model\n",
    "            if not is_zero_gas_guarantee_valid(isphs, 1e-3):\n",
    "                all_passed = False\n",
    "                print(f'{model_name}, augment {num_aug}, instance {n} might not be 0-GAS.')\n",
    "    if all_passed:\n",
    "        print(f'All {model_name} instances are 0-GAS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "#### Box/Violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "violinplot = False      # True -> Violinplot, False > Boxplot\n",
    "colorful = False        # If true, then the median lines are drawn in the model color\n",
    "\n",
    "use_types = ['sPHNN', 'cPHNN', 'PHNN', 'NODE']\n",
    "\n",
    "plot_rmse_data = {}\n",
    "for model_type, model_results in results.items():\n",
    "    if model_type not in use_types:\n",
    "        continue\n",
    "    plot_rmse_data[model_type] = {}\n",
    "    for num_aug, aug_results in model_results.items():\n",
    "        plot_rmse_data[model_type][num_aug] = [instance_result['test_rmse'] for instance_result in aug_results]\n",
    "\n",
    "n_results = len(plot_rmse_data)\n",
    "fig, axes = plt.subplots(1, n_results, figsize=(1.2*n_results, 3), dpi=PLOT_DPI, sharey=True)\n",
    "if n_results == 1:\n",
    "    axes = [axes]\n",
    "# fig.subplots_adjust(left=0.075, right=0.95, top=0.9, bottom=0.25)\n",
    "\n",
    "for n, (ax, (model_type, rmses)) in enumerate(zip(axes, plot_rmse_data.items())):\n",
    "\n",
    "    \n",
    "    if violinplot:\n",
    "        vp = ax.violinplot(rmses.values(), rmses.keys(), \n",
    "                  showextrema=True, showmeans=False, showmedians=True)\n",
    "        plt.setp(vp['cmedians'], color='red')\n",
    "\n",
    "        ax.set_xticks(list(rmses.keys()))\n",
    "    else:\n",
    "        bp = ax.boxplot(rmses.values(), notch=False, sym='+', vert=True, widths=0.7)\n",
    "        plt.setp(bp['medians'], color=model_style[model_type]['c'] if colorful else colors.theme_colors['lightblue'])\n",
    "\n",
    "        ax.set_xticklabels(rmses.keys())\n",
    "\n",
    "    # Add a horizontal grid to the plot, but make it very light in color\n",
    "    # so we can use it for reading data values but not be distracting\n",
    "    ax.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',\n",
    "                alpha=0.5)\n",
    "\n",
    "    ax.set(\n",
    "        axisbelow=True,  # Hide the grid behind plot objects\n",
    "        title=model_names[model_type], #f'{len(rmses[0])} {model_type}',\n",
    "        xlabel='Number of augmentations' if n==1 else '',\n",
    "        ylabel=R'$\\text{RMSE}\\,/\\,V$' if n==0 else '',\n",
    "        ylim=[0.9, 100],\n",
    "        yscale='log',\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "name_add_on = 'violinplot' if violinplot else 'boxplot'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom trajectory experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_eval = jnp.linspace(0, 5000, 1000)\n",
    "ys_eval = np.full((ts_eval.size, 2), np.nan)\n",
    "ys_eval[0,:] = 279.15\n",
    "m1 = 50\n",
    "m2 = 100\n",
    "\n",
    "us_eval = np.full((ts_eval.size, 1), 279.15)\n",
    "mask1 = np.logical_and(100<ts_eval, ts_eval<800)\n",
    "us_eval[mask1] = 400\n",
    "mask2 = ts_eval>=800\n",
    "us_eval[mask2] = 310\n",
    "us_eval = jnp.array(us_eval)\n",
    "\n",
    "prediction_data, true_data = get_prediction_statistics(ts_eval, ys_eval, us_eval, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_augs = max([len(x) for x in prediction_data.values()])\n",
    "\n",
    "fig, axes = plt.subplots(1, n_augs, sharey=True, figsize=(2*n_augs, 2), dpi=PLOT_DPI)\n",
    "axes = np.atleast_1d(axes)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.plot(true_data['ts'], true_data['us'].squeeze(), '--', c=colors.theme_colors['grey'], label=R'$T_\\text{oven}$')\n",
    "\n",
    "use_line = 'interquartile_mean'\n",
    "for model_type, model_plot_data in prediction_data.items():\n",
    "    # if model_type in ['PHNN', 'NODE']:\n",
    "    #     continue\n",
    "    for col, (aug_dim, aug_plot_data) in enumerate(model_plot_data.items()):\n",
    "\n",
    "        ax = axes[col]\n",
    "        line, = ax.plot(true_data['ts'], aug_plot_data['ys'][use_line][0,:,0], \n",
    "                        **model_style[model_type], label=model_names[model_type])\n",
    "        ax.fill_between(true_data['ts'], \n",
    "                        aug_plot_data['ys']['first_quartile'][0,:,0], \n",
    "                        aug_plot_data['ys']['third_quartile'][0,:,0], \n",
    "                        lw=0, alpha=0.2, zorder=1, color=line.get_color())\n",
    "        ax.set(\n",
    "            title=aug_dim,\n",
    "            ylabel='$T\\,/\\,K$' if col==0 else '',\n",
    "            ylim=[275,405],\n",
    "            xlabel='$t\\,/\\,s$',\n",
    "            xlim=[true_data['ts'].min(), true_data['ts'].max()],\n",
    "            xticks=[0, 3000],\n",
    "        )\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), \n",
    "           bbox_transform=fig.transFigure, fancybox=True, shadow=False, ncol=5)\n",
    "fig.suptitle('Number of augmented dimensions', y=1.1)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot predictions for a test trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_test_id = 9\n",
    "use_aug_dim = 3\n",
    "use_model_types = ['sPHNN', 'cPHNN', 'NODE', 'PHNN']\n",
    "\n",
    "ts_eval = test_data[0]\n",
    "ys_eval = test_data[1][use_test_id]\n",
    "us_eval = test_data[2][use_test_id]\n",
    "\n",
    "\n",
    "plot_data = dict(ts = ts_eval, ys=ys_eval, us=us_eval)\n",
    "for model_type in use_model_types:\n",
    "    plot_data[model_type] = {}\n",
    "    aug_results = results[model_type][use_aug_dim]\n",
    "    ys_preds = np.empty((len(aug_results), ts_eval.shape[0], 2))\n",
    "    for n, instance_result in enumerate(aug_results):\n",
    "        model = instance_result['wrapped_model']\n",
    "        ys_preds[n] = model(ts_eval, ys_eval[0], us_eval)\n",
    "\n",
    "    plot_data[model_type][use_aug_dim] = evaluation.get_statistics(ys_preds)\n",
    "use_line = 'interquartile_mean'\n",
    "\n",
    "\n",
    "ts_plot = plot_data['ts']\n",
    "ys_plot = plot_data['ys']\n",
    "us_plot = plot_data['us']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4), dpi=150)\n",
    "ax.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',\n",
    "                alpha=0.5)\n",
    "ax.plot(ts_plot, us_plot.squeeze(), c='grey', label=R'$T_{\\text{oven}}$')\n",
    "ax.plot(ts_plot, ys_plot[:,0], c='black', ls='--', label='$T_A$', marker='o', markevery=25)\n",
    "ax.plot(ts_plot, ys_plot[:,1], c='gray', ls='-.', label='$T_B$')\n",
    "\n",
    "for model_type in use_model_types:\n",
    "    metrics = plot_data[model_type][use_aug_dim]\n",
    "\n",
    "    line, = ax.plot(ts_plot, metrics[use_line][:,0], \n",
    "                    '-', c=model_style[model_type]['c'], alpha=0.8, \n",
    "                    label=f'{model_type} {use_line} prediction')\n",
    "    ax.fill_between(ts_plot, metrics['first_quartile'][:,0], metrics['third_quartile'][:,0], \n",
    "                    alpha=0.2, zorder=1, color=line.get_color(), \n",
    "                    label=f'{model_type} 25 to 75 quantile')\n",
    "    \n",
    "ax.set(\n",
    "    axisbelow=True,\n",
    "    title=f'Test trajectory {test_ids[use_test_id]}',\n",
    "    xlabel='Time [s]',\n",
    "    ylabel=R'T / K',\n",
    ")\n",
    "\n",
    "plt.legend(loc=(1.04, 0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
